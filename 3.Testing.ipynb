{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "745c5edd-70aa-4434-9e1e-7716c0d36632",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60553db2-a295-4fda-8445-c5067fe02e94",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import utils\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93fb2e64-675f-40de-a98a-df17aec07521",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Import the saved variables from the previous notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d884bc-abc6-4423-8cc3-d15c004141a0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%store -r"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c2de0b9-7ee7-4604-8ba3-d26cab044d22",
   "metadata": {},
   "source": [
    "### Comparing results with Amazon OpenSearch Service\n",
    "\n",
    "To understand how results are ranked, you can run queries with and without personalization, and compare the results. You can use the following Python code to run two different queries and output the results to two JSON files. The first method runs a query to generate results without personalization. The second runs a method that uses the plugin to re-rank results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b53ac0-09e2-4495-aed3-230ac4769c48",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "selected_weight=0.5\n",
    "user_id = \"12\" # Here we select a user id to compare the results\n",
    "search = \"Tom Cruise\" # We use a search Term to search our movies index which we created earlier\n",
    "\n",
    "query1 = \"\"\"{\n",
    "            \"size\": 20,\n",
    "            \"query\": {\n",
    "                \"multi_match\": {\n",
    "                    \"query\": \"%SearchText%\",\n",
    "                    \"fields\": [\"title\", \"plot\", \"genres\", \"directedBy\", \"starring\"]\n",
    "                }\n",
    "            }\n",
    "        }\"\"\"\n",
    "\n",
    "query2 = \"\"\"{\n",
    "            \"size\": 20,\n",
    "            \"query\": {\n",
    "                \"multi_match\": {\n",
    "                    \"query\": \"%SearchText%\",\n",
    "                    \"fields\": [\"title\", \"plot\", \"genres\", \"directedBy\", \"starring\"]\n",
    "                }\n",
    "            },\n",
    "            \"ext\": {\n",
    "                \"personalize_request_parameters\": {\n",
    "                    \"user_id\": \"%UserId%\"\n",
    "                }\n",
    "            }\n",
    "        }\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f2d170b-13fe-4538-a646-bdf79e2f4dda",
   "metadata": {},
   "source": [
    "We will fill in the placeholders in the query with the actual variable values that were defined earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8977b222-269b-4acc-9643-0904a8207e0d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "doc_str = re.sub(r\"%SearchText%\", search, query1)\n",
    "doc_str = re.sub(r\"%UserId%\", user_id, doc_str)\n",
    "\n",
    "doc_str_2 = re.sub(r\"%SearchText%\", search, query2)\n",
    "doc_str_2 = re.sub(r\"%UserId%\", user_id, doc_str_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f66e5ceb-e792-4f0a-b21a-517b151a1312",
   "metadata": {
    "tags": []
   },
   "source": [
    "We will load the interactions and items datasets into dataframes. These will be used to identify recent movies the user has watched, based on their interaction history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13acce42-b8ef-428b-b977-3888ea98e65e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "interactions_df = utils.get_interactions(root_dir)\n",
    "items_df = utils.get_items(root_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "881d146d-bf38-43e2-bf2f-af77f85fa45c",
   "metadata": {},
   "source": [
    "### Recent interactions for a user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b0c113-f52b-482a-a823-edc389ed3ee1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "utils.load_recent_movies(user_id, interactions_df, items_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c6487d-fa48-48dd-8af0-742c89b9ef19",
   "metadata": {},
   "source": [
    "### Execute the search queries against the OpenSearch domain.\n",
    "We will execute two queries - one without personalization and one with personalization - and compare the results. The first query is run without any personalization parameters. The second query is run by setting the personalize_request_parameters to the user_id as shown in the query 2 above, below is an excerpt.\n",
    "\n",
    "`\"ext\": {\n",
    "        \"personalize_request_parameters\": {\n",
    "            \"user_id\": \"%UserId%\"\n",
    "        }\n",
    "    }`\n",
    "\n",
    "\n",
    "This allows us to evaluate the difference when applying personalization versus not applying personalization. We created the search pipeline in the [2.Configure_Amazon_OpenSearch.ipynb](./2.Configure_Amazon_OpenSearch.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bfb5565-e3c7-4561-bfde-08ae6b6661f0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "res1 = utils.run_search(doc_str, HOST)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af097e8",
   "metadata": {},
   "source": [
    "Now we run the same query but this time we pass additional parameters such as the userid and the pipeline name to get personalized search results. Feel free to check the `run_search` in `utils.py` method which runs the actual query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fffe7c2a",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "res2 = utils.run_search(doc_str_2, HOST, \"intelligent_ranking\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "372166f8-8c3f-4001-a432-9c8abaa5e8a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "utils.compare_results(res1, res2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b487d3d-2173-44bc-b682-a351e312b380",
   "metadata": {},
   "source": [
    "#### Performing the search again with a modified weight parameter for the search pipeline\n",
    "You can experiment with different values of weight in the below cells and see the effect of Personalization on your queries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b2c0bc-1521-41d4-9298-42260cc7e37e",
   "metadata": {},
   "source": [
    "Updating the weight for the search pipeline (can be set from 0.0 to 1.0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4609d0a-208f-4498-90b9-ceb799a012df",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "utils.update_pipeline(\"intelligent_ranking\", \"1.0\", campaign_arn, role_arn_for_personalize, region, HOST, PORT)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "896c7e4c-32ab-4c4b-bb14-2c201c8a0cee",
   "metadata": {},
   "source": [
    "Re-executing the search query with an updated weight parameter configured for the search pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0989af73-8443-43e1-97c0-c814ec171cd8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "res2 = utils.run_search(doc_str_2, HOST, \"intelligent_ranking\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44bc2882-6552-44f3-850e-a97cc11bf6a3",
   "metadata": {},
   "source": [
    "Comparing results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51bac298-7d83-413b-a79a-527f44141bc6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "utils.compare_results(res1, res2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5abfbc1a-1f29-4176-b114-47018cc411fb",
   "metadata": {},
   "source": [
    "The weight parameter controls the balance between the OpenSearch relevance ranking and the Amazon Personalize personalized ranking. When the weight is set to 0.0, no personalization from Amazon Personalize is applied - the results are ranked solely based on OpenSearch relevance. \n",
    "\n",
    "As the weight is increased towards 1.0, more priority is given to the Amazon Personalize ranking scores over the OpenSearch relevance scores. A weight of 1.0 means the final ranking will be fully determined by the Amazon Personalize personalized ranking.\n",
    "\n",
    "So the closer the weight parameter is to 1.0, the more the results ranking will be biased towards the personalized ranking from Amazon Personalize over the relevance ranking from OpenSearch. This allows you to tune the level of personalization to find the right balance for your application.\n",
    "\n",
    "Comparing results with weight = 0.0 and weight = 1.0 clearly shows the two extremes of how the weighting parameter controls the influence of Amazon Personalize on the final ranked results returned to the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f54421-d1e1-4ef3-b457-108c25f810e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
