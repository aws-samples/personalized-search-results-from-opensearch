{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18a005f2-4db5-454e-a140-00071a557408",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "## Introduction <a class=\"anchor\" id=\"intro\"></a>\n",
    "[Back to top](#top)\n",
    "\n",
    "In Amazon Personalize, you start by creating a dataset group, which is a container for Amazon Personalize components. Your dataset group can be one of the following:\n",
    "\n",
    "A Domain dataset group, where you create preconfigured resources for different business domains and use cases, such as getting recommendations for similar videos (VIDEO_ON_DEMAND domain) or best-selling items (ECOMMERCE domain). You choose your business domain, import your data, and create recommenders. You use recommenders in your application to get recommendations.\n",
    "\n",
    "Use a [Domain dataset group](https://docs.aws.amazon.com/personalize/latest/dg/domain-dataset-groups.html) if you have a video on demand or e-commerce application and want Amazon Personalize to find the best configurations for your use cases. If you start with a Domain dataset group, you can also add custom resources such as solutions with solution versions trained with recipes for custom use cases.\n",
    "\n",
    "A [Custom dataset group](https://docs.aws.amazon.com/personalize/latest/dg/custom-dataset-groups.html), where you create configurable resources for custom use cases and batch recommendation workflows. You choose a recipe, train a solution version (model), and deploy the solution version with a campaign. You use a campaign in your application to get recommendations.\n",
    "\n",
    "Use a Custom dataset group if you don't have a video on demand or e-commerce application or want to configure and manage only custom resources, or want to get recommendations in a batch workflow. If you start with a Custom dataset group, you can't associate it with a domain later. Instead, create a new Domain dataset group.\n",
    "\n",
    "You can create and manage Domain dataset groups and Custom dataset groups with the AWS console, the AWS Command Line Interface (AWS CLI), or programmatically with the AWS SDKs.\n",
    "\n",
    "## Define your Use Case <a class=\"anchor\" id=\"usecase\"></a>\n",
    "[Back to top](#top)\n",
    "\n",
    "There are a few guidelines for scoping a problem suitable for Personalize. We recommend the values below as a starting point, although the [official limits](https://docs.aws.amazon.com/personalize/latest/dg/limits.html) are a little lower.\n",
    "\n",
    "* Authenticated users\n",
    "* At least 50 unique users\n",
    "* At least 100 unique items\n",
    "* At least 2 dozen interactions for each user \n",
    "\n",
    "Most of the time this is easily attainable, and if you are low in one category, you can often make up for it by having a larger number in another category.\n",
    "\n",
    "The user-item-iteraction data is key for getting started with the service. This means we need to look for use cases that generate that kind of data, a few common examples are:\n",
    "\n",
    "- Video-on-demand applications\n",
    "- E-commerce platforms\n",
    "\n",
    "Defining your use-case will inform what data and what type of data you need.\n",
    "\n",
    "In this example we are going to be creating:\n",
    "\n",
    "- Amazon Personalize Custom Campaign for a personalized ranked list of movies, for instance shelf/rail/carousel based on some information (director, location, superhero franchise, etc...) \n",
    "\n",
    "All of these will be created within the same dataset group and with the same input data.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Choose a Dataset or Data Source <a class=\"anchor\" id=\"source\"></a>\n",
    "[Back to top](#top)\n",
    "\n",
    "Regardless of the use case, the algorithms all share a base of learning on user-item-interaction data which is defined by 3 core attributes:\n",
    "\n",
    "1. **UserID** - The user who interacted\n",
    "2. **ItemID** - The item the user interacted with\n",
    "3. **Timestamp** - The time at which the interaction occurred\n",
    "\n",
    "To begin, we are going to use the latest MovieLens dataset, this dataset has over 25 million interactions and a rich collection of metadata for items. There is also a smaller version of this dataset, which can be used to shorten training times, while still incorporating the same capabilities as the full dataset. In this example we are going to use the smaller version of the dataset\n",
    "\n",
    "Generally speaking your data will not arrive in a perfect form for Personalize, and will take some modification to be structured correctly. This notebook guides you through all of that. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bccd053",
   "metadata": {},
   "source": [
    "### Install required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb02fba7",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install requests_auth_aws_sigv4\n",
    "!pip install requests\n",
    "!pip install opensearch-py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57161592",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430f0453-93ed-41e9-b36e-f3d1ce75dc40",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "import time\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import os\n",
    "from utils import create_s3_bucket, create_iam_role, upload_to_s3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c0bbe2e-1b0f-4eaf-9a4d-7b5c12b6ea1c",
   "metadata": {},
   "source": [
    "IPython extension to reload modules before executing user code, `autoreload` reloads modules automatically before entering the execution of code typed at the IPython prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05356a34-475a-4d98-9134-29a40e4d6422",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ce393b-c552-44a4-ae37-0752064e98fe",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Setup Region\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75e1f0d-fe62-4fb2-9e6f-fbe6084e3e47",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('/opt/ml/metadata/resource-metadata.json') as notebook_info:\n",
    "    data = json.load(notebook_info)\n",
    "    resource_arn = data['ResourceArn']\n",
    "    region = resource_arn.split(':')[3]\n",
    "print('region:', region)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6493884c-6422-4cde-b87f-f18fe1c79d16",
   "metadata": {},
   "source": [
    "## Create personalize client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab8f778-14a8-47a2-8df0-1bdd03cb0f12",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Configure the SDK to Personalize:\n",
    "personalize = boto3.client('personalize')\n",
    "personalize_runtime = boto3.client('personalize-runtime')\n",
    "print(\"We can communicate with Personalize!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aaf0389-19da-4fa1-a0c9-70c5e1f8183e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Download and Preprocess dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3beaa0ee-a492-4e83-989b-eb6274b575f2",
   "metadata": {},
   "source": [
    "### Download data\n",
    "We will download the dataset from the movielens website and unzip it in a new folder using the code below. If you want to use the full version of the dataset you can download it from here: http://files.grouplens.org/datasets/movielens/ml-25m.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a960d4-3fb5-4ac9-895a-6696cd603c50",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_dir = \"poc_data\"\n",
    "root_dir = data_dir + \"/ml-latest-small/\"\n",
    "\n",
    "if not os.path.exists(data_dir):\n",
    "    !mkdir $data_dir\n",
    "\n",
    "dataset_file = f\"{data_dir}/ml-latest-small.zip\"\n",
    "\n",
    "\n",
    "if not os.path.exists(dataset_file):\n",
    "    !cd $data_dir && wget http://files.grouplens.org/datasets/movielens/ml-latest-small.zip\n",
    "    !cd $data_dir && unzip ml-latest-small.zip\n",
    "    !ls $dataset_dir\n",
    "else:\n",
    "    print(dataset_file + \" already exists\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f22a04-035f-41f5-890a-4361c8bac936",
   "metadata": {},
   "source": [
    "### Create an Amazon S3 bucket\n",
    "Amazon S3 bucket names are globally unique. To create a unique bucket name, the code below will append the string `personalize-os-ranking` to your AWS account number. Then it creates a bucket with this name in the same region. Amazon Personalize needs to be able to read the contents of your S3 bucket. So we also add a bucket policy which allows that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "684a197c-2b9d-4e0f-8003-0b9fc1faefb0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bucket_name = create_s3_bucket(\"personalize-os-ranking\", region)\n",
    "bucket_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2457d17-28d4-4f97-a72d-cc7824b6c8a5",
   "metadata": {},
   "source": [
    "### Create an IAM role\n",
    "By default, the Personalize service does not have permission to access the data we uploaded into the S3 bucket in our account. In order to grant access to the Personalize service to read our CSVs, we need to set a Bucket Policy and create an IAM role that the Amazon Personalize service will assume. Let's set all of that up.\n",
    "\n",
    "Amazon Personalize needs the ability to assume roles in AWS in order to have the permissions to execute certain tasks. Let's create an IAM role and attach the required policies to it. The code below attaches very permissive policies; please use more restrictive policies for any production application.\n",
    "\n",
    "The create_iam_role method is defined in the `utils.py` file which has other reusable functions defined in it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "973a38df-9522-4829-9a7f-7dca416dbae5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "role_arn = create_iam_role(\"personalize-os-ranking-role\", bucket_name)\n",
    "role_arn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb42775e-4710-43c7-8d72-3c94a718f869",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Create Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe23b93d-a856-4bdf-9a23-72a1f4e455d2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "resource_name = 'personalize-os-ranking'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93149c76-61e3-417d-8e30-056674924c66",
   "metadata": {},
   "source": [
    "### Create DSG\n",
    "\n",
    "The highest level of isolation and abstraction with Amazon Personalize is a dataset group. Information stored within one of these dataset groups has no impact on any other dataset group or models created from one - they are completely isolated. This allows you to run many experiments and is part of how we keep your models private and fully trained only on your data.\n",
    "\n",
    "Before importing the data prepared earlier, there needs to be a dataset group and a dataset added to it that handles the interactions.\n",
    "\n",
    "Dataset groups can house the following types of information:\n",
    "\n",
    "- User-item-interactions\n",
    "- Event streams (real-time interactions)\n",
    "- User metadata\n",
    "- Item metadata\n",
    "\n",
    "We need to create the dataset group that will contain our three datasets.\n",
    "\n",
    "\n",
    "The following cell will create a new dataset group with the name `personalize-os-ranking`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0873c4-9a3c-4816-ba4d-394978226328",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    create_dataset_group_response = personalize.create_dataset_group(\n",
    "        name = resource_name\n",
    "    )\n",
    "    dataset_group_arn = create_dataset_group_response['datasetGroupArn']\n",
    "    print('dataset_group_arn: {}'.format(dataset_group_arn))\n",
    "except personalize.exceptions.ResourceAlreadyExistsException:\n",
    "    print('You already created this datasetgroup.')\n",
    "    dsgs = personalize.list_dataset_groups(\n",
    "        maxResults=100\n",
    "    )\n",
    "    \n",
    "    for dsg in dsgs['datasetGroups']:\n",
    "        #print(dsg)\n",
    "        if dsg['name'] == resource_name:\n",
    "            dataset_group_arn = dsg['datasetGroupArn']\n",
    "            print(f\"Using existing dsg: {dataset_group_arn}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f49b69-cad7-4090-8281-83a2bfceb74f",
   "metadata": {},
   "source": [
    "#### Wait for Dataset Group to Have ACTIVE Status\n",
    "Before we can use the Dataset Group in any items below it must be active. This can take a minute or two. Execute the cell below and wait for it to show the ACTIVE status. It checks the status of the dataset group every 60 seconds, up to a maximum of 3 hours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9526c95-8449-4a7c-95c3-5b74c066df6c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "status = None\n",
    "max_time = time.time() + 3*60*60 # 3 hours\n",
    "while time.time() < max_time:\n",
    "    describe_dataset_group_response = personalize.describe_dataset_group(\n",
    "        datasetGroupArn = dataset_group_arn\n",
    "    )\n",
    "    status = describe_dataset_group_response[\"datasetGroup\"][\"status\"]\n",
    "    print(\"DatasetGroup: {}\".format(status))\n",
    "    \n",
    "    if status == \"ACTIVE\" or status == \"CREATE FAILED\":\n",
    "        break\n",
    "        \n",
    "    time.sleep(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b2afd15-80f5-4d72-8814-6b54c1253744",
   "metadata": {},
   "source": [
    "## Create Schemas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3941d314-e11e-4a0f-baa3-e03b901e62f3",
   "metadata": {},
   "source": [
    "### Create Interaction Schema\n",
    "Amazon Personalize requires a schema for each dataset, so it can map the columns in our CSVs to fields for model training. Each schema is declared in JSON using the [Apache Avro](https://avro.apache.org/) format.\n",
    "\n",
    "First, define a schema to tell Amazon Personalize what type of dataset you are uploading. There are several reserved and mandatory keywords required in the schema, based on the type of dataset. More detailed information can be found in the [documentation](https://docs.aws.amazon.com/personalize/latest/dg/how-it-works-dataset-schema.html).\n",
    "\n",
    "Here, you will create a schema for interactions data, which requires the `USER_ID`, `ITEM_ID`, and `TIMESTAMP` fields. These must be defined in the same order in the schema as they appear in the dataset.\n",
    "\n",
    "The interactions dataset has three required columns: `ITEM_ID`, `USER_ID`, and `TIMESTAMP`. The `TIMESTAMP` represents when the user interacted with an item and must be expressed in Unix timestamp format (seconds). For this dataset we also have an `EVENT_TYPE` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f51760b2-370f-4d52-9760-57fa3cf3d490",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "interaction_schema = {\n",
    "    \"type\": \"record\",\n",
    "    \"name\": \"Interactions\",\n",
    "    \"namespace\": \"com.amazonaws.personalize.schema\",\n",
    "    \"fields\": [\n",
    "        {\n",
    "            \"name\": \"USER_ID\",\n",
    "            \"type\": \"string\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"ITEM_ID\",\n",
    "            \"type\": \"string\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"TIMESTAMP\",\n",
    "            \"type\": \"long\"\n",
    "        },\n",
    "        { \n",
    "            \"name\": \"EVENT_TYPE\",\n",
    "            \"type\": \"string\"\n",
    "        }\n",
    "    ],\n",
    "    \"version\": \"1.0\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c58c3a2c-ea64-480b-a08f-90a3e044ff0b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    interaction_schema_response = personalize.create_schema(\n",
    "        name = f\"{resource_name}-interaction-schema\",\n",
    "        schema = json.dumps(interaction_schema)\n",
    "        )\n",
    "    interaction_schema_arn = interaction_schema_response['schemaArn']\n",
    "    print('interaction_schema_arn:\\n', interaction_schema_arn)\n",
    "except personalize.exceptions.ResourceAlreadyExistsException:\n",
    "    print('You already created this schema.')\n",
    "    schemas = personalize.list_schemas(maxResults=100)['schemas']\n",
    "    for schema_response in schemas:\n",
    "        if schema_response['name'] == f\"{resource_name}-interaction-schema\":\n",
    "            interaction_schema_arn = schema_response['schemaArn']\n",
    "            print(f\"Using existing schema: {interaction_schema_arn}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e8f88c1-d360-4de8-9924-84b667283cd6",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Create User Schema\n",
    "\n",
    "Here, you will create a schema for user data, which requires the `USER_ID`, and an additional metadata field, in this case `GENDER`. These must be defined in the same order in the schema as they appear in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "823c11ab-b89f-4762-aa65-546d10dcf57b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "users_schema = {\n",
    "    \"type\": \"record\",\n",
    "    \"name\": \"Users\",\n",
    "    \"namespace\": \"com.amazonaws.personalize.schema\",\n",
    "    \"fields\": [\n",
    "        {\n",
    "            \"name\": \"USER_ID\",\n",
    "            \"type\": \"string\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"GENDER\",\n",
    "            \"type\": \"string\",\n",
    "            \"categorical\": True\n",
    "        }\n",
    "    ],\n",
    "    \"version\": \"1.0\"\n",
    "}\n",
    "    \n",
    "try:\n",
    "    create_schema_response = personalize.create_schema(\n",
    "        name = f\"{resource_name}-user-schema\",\n",
    "        schema = json.dumps(users_schema)\n",
    "    )\n",
    "    print(json.dumps(create_schema_response, indent=2))\n",
    "    users_schema_arn = create_schema_response['schemaArn']\n",
    "except personalize.exceptions.ResourceAlreadyExistsException:\n",
    "    print('You already created this schema.')\n",
    "    schemas = personalize.list_schemas(maxResults=100)['schemas']\n",
    "    for schema_response in schemas:\n",
    "        if schema_response['name'] == f\"{resource_name}-user-schema\":\n",
    "            users_schema_arn = schema_response['schemaArn']\n",
    "            print(f\"Using existing schema: {users_schema_arn}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d72b99-a0c9-4ba5-af46-7afd01ddf803",
   "metadata": {},
   "source": [
    "### Create Items Schema\n",
    "\n",
    "Here, you will create a schema for item metadata data, and we define the `ITEM_ID`, `GENRES`, `YEAR`, and `CREATION_TIMESTAMP` fields. These must be defined in the same order in the schema as they appear in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc8d7e8-f749-4203-b209-37aa2d8aa982",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "items_schema = {\n",
    "    \"type\": \"record\",\n",
    "    \"name\": \"Items\",\n",
    "    \"namespace\": \"com.amazonaws.personalize.schema\",\n",
    "    \"fields\": [\n",
    "        {\n",
    "            \"name\": \"ITEM_ID\",\n",
    "            \"type\": \"string\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"GENRES\",\n",
    "            \"type\": \"string\",\n",
    "            \"categorical\": True\n",
    "        },{\n",
    "            \"name\": \"YEAR\",\n",
    "            \"type\": \"int\",\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"CREATION_TIMESTAMP\",\n",
    "            \"type\": \"long\",\n",
    "        }\n",
    "    ],\n",
    "    \"version\": \"1.0\"\n",
    "}\n",
    "    \n",
    "try:\n",
    "    create_schema_response = personalize.create_schema(\n",
    "        name = f\"{resource_name}-item-schema\",\n",
    "        schema = json.dumps(items_schema)\n",
    "    )\n",
    "    items_schema_arn = create_schema_response['schemaArn']\n",
    "    print(json.dumps(create_schema_response, indent=2))\n",
    "except personalize.exceptions.ResourceAlreadyExistsException:\n",
    "    print('You already created this schema.')\n",
    "    schemas = personalize.list_schemas(maxResults=100)['schemas']\n",
    "    for schema_response in schemas:\n",
    "        if schema_response['name'] == f\"{resource_name}-item-schema\":\n",
    "            items_schema_arn = schema_response['schemaArn']\n",
    "            print(f\"Using existing schema: {items_schema_arn}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da4cc2c2-104a-4868-9f26-7df9275e696e",
   "metadata": {},
   "source": [
    "### Prepare the Interactions data and Create Interaction dataset file\n",
    "\n",
    "Since this is a dataset of an explicit feedback movie ratings, it includes movies rated from 1 to 5. We want to include only moves that were \"liked\" by the users, and simulate a dataset of data that would be gathered by a VOD platform. In order to do that, we will filter out all interactions under 2 out of 5, and create two event types: \"Click\" and \"Watch\". We will then assign all movies rated 2 and above as \"Click\" and movies rated 4 and above as both \"Click\" and \"Watch\".\n",
    "\n",
    "Note that for a real data set you would actually model based on implicit feedback such as clicks, watches and/or explicit feedback such as ratings, likes etc.\n",
    "\n",
    "\n",
    "Amazon Personalize has default column names for users, items, and timestamp. These default column names are `USER_ID`, `ITEM_ID`, `TIMESTAMP`. The final modification to the dataset is to replace the existing column headers with the default headers.\n",
    "\n",
    "Finally, we upload the file to Amazon S3 bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d0c7feb-33b5-45ac-9490-2727bb6793c8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "original_data = pd.read_csv(root_dir +'/ratings.csv')\n",
    "original_data.head(5)\n",
    "\n",
    "arb_time_stamp = original_data.iloc[50]['timestamp']\n",
    "\n",
    "watched_df = original_data.copy()\n",
    "watched_df = watched_df[watched_df['rating'] > 3]\n",
    "watched_df = watched_df[['userId', 'movieId', 'timestamp']]\n",
    "watched_df['EVENT_TYPE']='Watch'\n",
    "print(watched_df.head())\n",
    "print(watched_df.shape)\n",
    "clicked_df = original_data.copy()\n",
    "clicked_df = clicked_df[clicked_df['rating'] > 1]\n",
    "clicked_df = clicked_df[['userId', 'movieId', 'timestamp']]\n",
    "clicked_df['EVENT_TYPE']='Click'\n",
    "clicked_df.head()\n",
    "print(clicked_df.head())\n",
    "print(clicked_df.shape)\n",
    "interactions_df = clicked_df.copy()\n",
    "interactions_df = pd.concat([interactions_df,watched_df], axis = 0)\n",
    "print(interactions_df.shape)\n",
    "interactions_df.sort_values(\"timestamp\", axis = 0, ascending = True, \n",
    "                 inplace = True, na_position ='last') \n",
    "interactions_df.rename(columns = {'userId':'USER_ID', 'movieId':'ITEM_ID', \n",
    "                              'timestamp':'TIMESTAMP'}, inplace = True) \n",
    "interactions_filename = \"interactions.csv\"\n",
    "interactions_df.to_csv((root_dir +\"/\"+interactions_filename), index=False, float_format='%.0f')\n",
    "\n",
    "\n",
    "upload_to_s3(root_dir +\"/\"+interactions_filename, bucket_name, interactions_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f524dc3f-b81e-4ef5-a20e-7b9cdb394133",
   "metadata": {},
   "source": [
    "### Prepare the Item Metadata and Create Items dataset file\n",
    "\n",
    "Next we load the data and confirm the data is in a good state.\n",
    "\n",
    "Next, open the `movies.csv` file and process this dataset. This is a pretty small dataset of just the movieId, title and the list of genres that are applicable to each entry. However, there is additional data available in the movielens dataset. For instance the title includes the year of the movies release. Let's make that another column of metadata. We then remove null values.\n",
    "\n",
    "From an item metadata perspective, we only want to include information that is relevant to training a model and/or filtering results, so we will drop the title column, and keep the genre information.\n",
    "\n",
    "Finally, we will add a new dataframe to help us generate a creation timestamp. If you don’t provide the CREATION_TIMESTAMP for an item, the model infers this information from the interaction dataset and uses the timestamp of the item’s earliest interaction as its corresponding release date. If an item doesn’t have an interaction, its release date is set as the timestamp of the latest interaction in the training set, and it is considered a new item.\n",
    "\n",
    "For the current example we are selecting a today's date as the creation timestamp because the actual creation timestamp is unknown. In your use-case, please provide the appropriate creation timestamp for the item. This can be when the item was added to your platform.\n",
    "\n",
    "Amazon Personalize has a default column for `ITEM_ID` that will map to our `movieId`. We will flesh out more information by specifying `GENRE` as well. Finally, we save the file and upload it to Amazon S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c798d0-dfe0-4c28-872a-29f1af142627",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "original_data = pd.read_csv(root_dir +'/movies.csv')\n",
    "original_data\n",
    "original_data['year'] = original_data['title'].str.extract('.*\\((.*)\\).*',expand = False)\n",
    "original_data.head(5)\n",
    "original_data = original_data.dropna(axis=0)\n",
    "original_data.isnull().sum()\n",
    "itemmetadata_df = original_data.copy()\n",
    "itemmetadata_df = itemmetadata_df[['movieId', 'genres', 'year']]\n",
    "itemmetadata_df.head()\n",
    "ts = datetime(2022, 1, 1, 0, 0).strftime('%s')\n",
    "\n",
    "\n",
    "itemmetadata_df['CREATION_TIMESTAMP'] = ts\n",
    "itemmetadata_df.rename(columns = {'genres':'GENRES', 'movieId':'ITEM_ID', 'year':'YEAR'}, inplace = True) \n",
    "items_filename = \"item-meta.csv\"\n",
    "itemmetadata_df.to_csv((root_dir+\"/\"+items_filename), index=False, float_format='%.0f')\n",
    "\n",
    "upload_to_s3(root_dir +\"/\"+items_filename, bucket_name, items_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "112fe580-659b-4bda-ad48-781b1dc5de25",
   "metadata": {},
   "source": [
    "### Create Users dataset file\n",
    "\n",
    "This dataset does not have any user metadata, so we will create a fake metadata field. The current dataset does not contain additional user information. For this example, we'll randomly assign a gender to the users with equal probability of male and female. We finally save the file and upload it to Amazon S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258e0bd8-62ba-4e74-844f-7f145ee6d534",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get all unique user ids from the interaction dataset\n",
    "user_ids = interactions_df['USER_ID'].unique()\n",
    "user_data = pd.DataFrame()\n",
    "user_data[\"USER_ID\"]= user_ids\n",
    "user_data\n",
    "possible_genders = ['female', 'male']\n",
    "random = np.random.choice(possible_genders, len(user_data.index), p=[0.5, 0.5])\n",
    "\n",
    "user_data[\"GENDER\"] = random\n",
    "user_data\n",
    "# Saving the data as a CSV file\n",
    "users_filename = \"users.csv\"\n",
    "user_data.to_csv((root_dir+\"/\"+users_filename), index=False, float_format='%.0f')\n",
    "\n",
    "upload_to_s3(root_dir +\"/\"+users_filename, bucket_name, users_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d284476d-f465-4515-af79-29be5ca556ac",
   "metadata": {},
   "source": [
    "### Create Interaction dataset\n",
    "With a schema created, you can create an interactions dataset within the dataset group. Note that this does not load the data yet, but creates a schema of what the data looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c22b868a-09b4-4670-a6e8-90469a3a293f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    interactions_dataset_response = personalize.create_dataset(\n",
    "        datasetType = 'INTERACTIONS',\n",
    "        datasetGroupArn = dataset_group_arn,\n",
    "        schemaArn = interaction_schema_arn,\n",
    "        name = resource_name\n",
    "    )\n",
    "    interaction_dataset_arn = interactions_dataset_response['datasetArn']\n",
    "    print('interaction_dataset_arn:\\n', interaction_dataset_arn)\n",
    "\n",
    "except personalize.exceptions.ResourceAlreadyExistsException:\n",
    "    print('You already created this dataset.',dataset_group_arn)\n",
    "    datasets = personalize.list_datasets(\n",
    "            datasetGroupArn=dataset_group_arn,\n",
    "            maxResults=100\n",
    "        )\n",
    "    \n",
    "    for dataset in datasets['datasets']:\n",
    "        \n",
    "        if (dataset['name'] == resource_name) & (dataset['datasetType'] == \"INTERACTIONS\") :\n",
    "            interaction_dataset_arn = dataset['datasetArn']\n",
    "            print(f\"Using Interaction dataset: {interaction_dataset_arn}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8623378f-3b98-4932-96c0-fddc68916dff",
   "metadata": {},
   "source": [
    "### Create Users dataset\n",
    "\n",
    "With a schema created, you can create an users dataset within the dataset group. Note that this does not load the data yet, but creates a schema of what the data looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82fede83-0825-466e-9928-dcd8462fbce1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    dataset_type = \"USERS\"\n",
    "    create_dataset_response = personalize.create_dataset(\n",
    "        name = resource_name,\n",
    "        datasetType = dataset_type,\n",
    "        datasetGroupArn = dataset_group_arn,\n",
    "        schemaArn = users_schema_arn\n",
    "    )\n",
    "\n",
    "    users_dataset_arn = create_dataset_response['datasetArn']\n",
    "    print(json.dumps(create_dataset_response, indent=2))\n",
    "\n",
    "except personalize.exceptions.ResourceAlreadyExistsException:\n",
    "    print('You already created this dataset.')\n",
    "    datasets = personalize.list_datasets(\n",
    "            datasetGroupArn=dataset_group_arn,\n",
    "            maxResults=100\n",
    "        )\n",
    "    for dataset in datasets['datasets']:\n",
    "        \n",
    "        if (dataset['name'] == resource_name) & (dataset['datasetType'] == \"USERS\") :\n",
    "            users_dataset_arn = dataset['datasetArn']\n",
    "            print(f\"Using Users dataset: {users_dataset_arn}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e183de7-36a7-4fad-99f2-e12cfa066da4",
   "metadata": {},
   "source": [
    "### Create Items dataset\n",
    "\n",
    "With a schema created, you can create an items dataset within the dataset group. Note that this does not load the data yet, but creates a schema of what the data looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc8aa9e-d45d-45f3-bdca-5434bbcfad7e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    dataset_type = \"ITEMS\"\n",
    "    create_dataset_response = personalize.create_dataset(\n",
    "        name = resource_name,\n",
    "        datasetType = dataset_type,\n",
    "        datasetGroupArn = dataset_group_arn,\n",
    "        schemaArn = items_schema_arn\n",
    "    )\n",
    "\n",
    "    items_dataset_arn = create_dataset_response['datasetArn']\n",
    "    print(json.dumps(create_dataset_response, indent=2))\n",
    "    \n",
    "except personalize.exceptions.ResourceAlreadyExistsException:\n",
    "    print('You already created this dataset.')\n",
    "    datasets = personalize.list_datasets(\n",
    "            datasetGroupArn=dataset_group_arn,\n",
    "            maxResults=100\n",
    "        )\n",
    "    for dataset in datasets['datasets']:\n",
    "        if (dataset['name'] == resource_name) & (dataset['datasetType'] == \"ITEMS\") :\n",
    "            items_dataset_arn = dataset['datasetArn']\n",
    "            print(f\"Using Items dataset: {items_dataset_arn}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10757cc4-ca6a-400c-afba-0481807ec99e",
   "metadata": {},
   "source": [
    "#### Wait for creation\n",
    "\n",
    "Let's wait until all the datasets have been created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f883fd-c95d-4bc3-95fa-1a8afe0fd15a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "max_time = time.time() + 3*60*60 # 3 hours\n",
    "while time.time() < max_time:\n",
    "    describe_dataset_response = personalize.describe_dataset(\n",
    "        datasetArn = interaction_dataset_arn\n",
    "    )\n",
    "    status = describe_dataset_response[\"dataset\"][\"status\"] \n",
    "    print(\"{} : {}\".format(interaction_dataset_arn, status))\n",
    "    if status == \"ACTIVE\" or status == \"CREATE FAILED\":\n",
    "        break\n",
    "    time.sleep(10)\n",
    "    \n",
    "while time.time() < max_time:\n",
    "    describe_dataset_response = personalize.describe_dataset(\n",
    "        datasetArn = items_dataset_arn\n",
    "    )\n",
    "    status =  describe_dataset_response[\"dataset\"]['status']\n",
    "    print(\"{} : {}\".format(items_dataset_arn, status))\n",
    "    \n",
    "    \n",
    "    if status == \"ACTIVE\" or status == \"CREATE FAILED\":\n",
    "        break\n",
    "        \n",
    "    time.sleep(60)\n",
    "    \n",
    "while time.time() < max_time:\n",
    "    describe_dataset_response = personalize.describe_dataset(\n",
    "        datasetArn = users_dataset_arn\n",
    "    )\n",
    "    status =  describe_dataset_response[\"dataset\"]['status']\n",
    "    print(\"{} : {}\".format(users_dataset_arn, status))\n",
    "    \n",
    "    if status == \"ACTIVE\" or status == \"CREATE FAILED\":\n",
    "        break\n",
    "        \n",
    "    time.sleep(60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ccd5e6b-f992-463e-a287-27a9da214261",
   "metadata": {},
   "source": [
    "### Import Interactions data\n",
    "\n",
    "Earlier you created the dataset group and the interactions dataset to house your information, so now you will execute an import job that will load the interactions data from the S3 bucket into the Amazon Personalize dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f47c55bf-c236-4770-82e4-0ab960c189a7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    \n",
    "    dsImportJobName = resource_name + 'Interactions'\n",
    "\n",
    "    interactions_dij_response = personalize.create_dataset_import_job(\n",
    "        jobName =  dsImportJobName,\n",
    "        datasetArn = interaction_dataset_arn,\n",
    "        dataSource = {\n",
    "            \"dataLocation\": \"s3://{}/{}\".format(bucket_name, interactions_filename)\n",
    "        },\n",
    "        roleArn = role_arn\n",
    "    )\n",
    "\n",
    "    interactions_dij_arn = interactions_dij_response['datasetImportJobArn']\n",
    "    print('interactions_dij_arn: ', interactions_dij_arn)\n",
    "    \n",
    "except personalize.exceptions.ResourceAlreadyExistsException:\n",
    "    print('You already created this dataset import job.')\n",
    "    dijs = personalize.list_dataset_import_jobs(\n",
    "                datasetArn=interaction_dataset_arn,\n",
    "                maxResults=100\n",
    "            )\n",
    "    for dij in dijs['datasetImportJobs']:\n",
    "        if dij['jobName'] == dsImportJobName:\n",
    "            interactions_dij_arn = dij['datasetImportJobArn']\n",
    "            print(f\"Using Interactions dataset: {interactions_dij_arn}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "475602c4-0d6c-471d-bdaf-69a274fd60e2",
   "metadata": {},
   "source": [
    "### Import Items data\n",
    "\n",
    "Earlier you created the dataset group and the items dataset to house your information, now you will execute an import job that will load the item data from the S3 bucket into the Amazon Personalize dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42615d3-3497-41e6-964b-aded2311a89b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    create_dataset_import_job_response = personalize.create_dataset_import_job(\n",
    "        jobName = f\"{resource_name}Items\",\n",
    "        datasetArn = items_dataset_arn,\n",
    "        dataSource = {\n",
    "            \"dataLocation\": \"s3://{}/{}\".format(bucket_name, items_filename)\n",
    "        },\n",
    "        roleArn = role_arn\n",
    "    )\n",
    "\n",
    "    items_dataset_import_job_arn = create_dataset_import_job_response['datasetImportJobArn']\n",
    "    print(json.dumps(create_dataset_import_job_response, indent=2))\n",
    "    \n",
    "except personalize.exceptions.ResourceAlreadyExistsException:\n",
    "    print('You already created this dataset import job.')\n",
    "    dijs = personalize.list_dataset_import_jobs(\n",
    "                datasetArn=items_dataset_arn,\n",
    "                maxResults=100\n",
    "            )\n",
    "    for dij in dijs['datasetImportJobs']:\n",
    "        if dij['jobName'] == f\"{resource_name}Items\":\n",
    "            items_dataset_import_job_arn = dij['datasetImportJobArn']\n",
    "            print(f\"Using Items dataset: {items_dataset_import_job_arn}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f78564b8-a02f-4f17-a632-504d4302d62d",
   "metadata": {},
   "source": [
    "### Import Users dataset\n",
    "\n",
    "Earlier you created the dataset group and the users dataset to house your information, now you will execute an import job that will load the user data from the S3 bucket into the Amazon Personalize dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9880cce-e148-4edb-9920-4504f7e5aa82",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    create_dataset_import_job_response = personalize.create_dataset_import_job(\n",
    "        jobName = f\"{resource_name}Users\",\n",
    "        datasetArn = users_dataset_arn,\n",
    "        dataSource = {\n",
    "            \"dataLocation\": \"s3://{}/{}\".format(bucket_name, users_filename)\n",
    "        },\n",
    "        roleArn = role_arn\n",
    "    )\n",
    "\n",
    "    users_dataset_import_job_arn = create_dataset_import_job_response['datasetImportJobArn']\n",
    "    print(json.dumps(create_dataset_import_job_response, indent=2))\n",
    "    \n",
    "except personalize.exceptions.ResourceAlreadyExistsException:\n",
    "    print('You already created this dataset import job.')\n",
    "    dijs = personalize.list_dataset_import_jobs(\n",
    "                datasetArn=users_dataset_arn,\n",
    "                maxResults=100\n",
    "            )\n",
    "    for dij in dijs['datasetImportJobs']:\n",
    "        if dij['jobName'] == f\"{resource_name}Users\":\n",
    "            users_dataset_import_job_arn = dij['datasetImportJobArn']\n",
    "            print(f\"Using Users dataset: {users_dataset_import_job_arn}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c551fb-bcc0-4bf7-89f1-0e9781d8dcfa",
   "metadata": {},
   "source": [
    "#### Wait for creation\n",
    "\n",
    "Before we can use the dataset, the import job must be active. Execute the cell below and wait for it to show the ACTIVE status. It checks the status of the import job every minute, up to a maximum of 6 hours.\n",
    "\n",
    "Importing the data can take some time, depending on the size of the dataset. In this workshop, the data import job should take around 15 minutes. While you're waiting you can learn more about Datasets and Schemas in [the documentation](https://docs.aws.amazon.com/personalize/latest/dg/how-it-works-dataset-schema.html). We need to wait for the data imports to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e55e60b-b943-498c-bb47-b17d2256112e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "max_time = time.time() + 3*60*60 # 3 hours\n",
    "while time.time() < max_time:\n",
    "    describe_dij_response = personalize.describe_dataset_import_job(\n",
    "        datasetImportJobArn = interactions_dij_arn\n",
    "    )\n",
    "    dataset_import_job = describe_dij_response[\"datasetImportJob\"]\n",
    "    if \"latestDatasetImportJobRun\" not in dataset_import_job:\n",
    "        status = dataset_import_job[\"status\"]\n",
    "    else:\n",
    "        status = describe_dij_response[\"latestDatasetImportJobRun\"][\"status\"] \n",
    "    print(\"{} : {}\".format(interactions_dij_arn, status))\n",
    "    if status == \"ACTIVE\" or status == \"CREATE FAILED\":\n",
    "        break\n",
    "    time.sleep(10)\n",
    "    \n",
    "while time.time() < max_time:\n",
    "    describe_dataset_import_job_response = personalize.describe_dataset_import_job(\n",
    "        datasetImportJobArn = items_dataset_import_job_arn\n",
    "    )\n",
    "    status = describe_dataset_import_job_response[\"datasetImportJob\"]['status']\n",
    "    print(\"{} : {}\".format(items_dataset_import_job_arn, status))\n",
    "    \n",
    "    if status == \"ACTIVE\" or status == \"CREATE FAILED\":\n",
    "        break\n",
    "        \n",
    "    time.sleep(60)\n",
    "    \n",
    "while time.time() < max_time:\n",
    "    describe_dataset_import_job_response = personalize.describe_dataset_import_job(\n",
    "        datasetImportJobArn = users_dataset_import_job_arn\n",
    "    )\n",
    "    status = describe_dataset_import_job_response[\"datasetImportJob\"]['status']\n",
    "    print(\"{} : {}\".format(users_dataset_import_job_arn, status))\n",
    "    \n",
    "    if status == \"ACTIVE\" or status == \"CREATE FAILED\":\n",
    "        break\n",
    "        \n",
    "    time.sleep(60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "197fe70f-cbed-48ef-8e95-ec0fdd766440",
   "metadata": {},
   "source": [
    "### Create Solutions\n",
    "\n",
    "Some use cases require a custom implementation, in this example we will use the `aws-personalized-ranking recipe`.\n",
    "\n",
    "In Amazon Personalize, a specific variation of an algorithm is called a recipe. Different recipes are suitable for different situations. A trained model is called a solution, and each solution can have many versions that relate to a given volume of data when the model was trained.\n",
    "\n",
    "Personalized Ranking is an interesting application of HRNN. Instead of just recommending what is most probable for the user in question, this algorithm takes in a list of items as well as a user. The items are then returned back in the order of most probable relevance for the user. The use case here is for filtering on unique categories that you do not have item metadata to create a filter, or when you have a broad collection that you would like better ordered for a particular user.\n",
    "\n",
    "For our use case, using the MovieLens data, we could imagine that a Video on Demand application may want to create a shelf of comic book movies, or movies by a specific director. We can generate these lists based on metadata we have. We would use personalized ranking to re-order the list of movies for each user. \n",
    "\n",
    "First you create a solution using the recipe. Although you provide the dataset ARN in this step, the model is not yet trained. See this as an identifier instead of a trained model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea8a7aa5-3b56-4115-9120-757a6f335c56",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    recipe_arn = \"arn:aws:personalize:::recipe/aws-personalized-ranking\"\n",
    "\n",
    "    create_solution_response = personalize.create_solution(\n",
    "        name = resource_name,\n",
    "        recipeArn = recipe_arn,\n",
    "        datasetGroupArn = dataset_group_arn)\n",
    "    solution_arn = create_solution_response[\"solutionArn\"]\n",
    "    print('solution arn:', solution_arn)\n",
    "\n",
    "except personalize.exceptions.ResourceAlreadyExistsException:\n",
    "    print('You already created this Solution.')\n",
    "    solutions = personalize.list_solutions(\n",
    "                datasetGroupArn=dataset_group_arn,\n",
    "                maxResults=100\n",
    "            )\n",
    "    for solution in solutions['solutions']:\n",
    "        if solution['name'] == resource_name:\n",
    "            solution_arn = solution['solutionArn']\n",
    "            print(f\"Solution Arn: {solution_arn}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a07eff07-8094-434b-830a-c3f052b51151",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Wait for creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a10ecd3-0890-434e-9643-a0908f5b1800",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "max_time = time.time() + 3*60*60 # 3 hours\n",
    "while time.time() < max_time:\n",
    "    describe_solution_response = personalize.describe_solution(\n",
    "        solutionArn = solution_arn\n",
    "    )\n",
    "    status = describe_solution_response[\"solution\"][\"status\"] \n",
    "    print(\"{} : {}\".format(solution_arn, status))\n",
    "    if status == \"ACTIVE\" or status == \"CREATE FAILED\":\n",
    "        break\n",
    "    time.sleep(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a15da5-b47b-443d-ab88-e5e171e5d855",
   "metadata": {},
   "source": [
    "### Create Solution Version\n",
    "\n",
    "Once you have a solution, you need to create a version in order to complete the model training. The training can take a while to complete, upwards of 25 minutes, and an average of 35 minutes for this recipe with our dataset. Normally, we would use a while loop to poll until the task is completed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "753e2cb3-b045-4383-a83b-af02fe1803ec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    create_sv_response = personalize.create_solution_version(\n",
    "            name=f\"{resource_name}-solution-version\",\n",
    "            solutionArn = solution_arn,\n",
    "            trainingMode = 'FULL'\n",
    "        )\n",
    "    sv_arn = create_sv_response[\"solutionVersionArn\"]\n",
    "    print('solutionVersionArn:', sv_arn)\n",
    "\n",
    "except personalize.exceptions.ResourceAlreadyExistsException:\n",
    "    print('You already created this Solution version.')\n",
    "    solution_versions = personalize.list_solution_versions(maxResults=100)['solutionVersions']\n",
    "    for solution_version in solution_versions:\n",
    "        print(solution_version['solutionVersionArn'])\n",
    "        arn = solution_version['solutionVersionArn']\n",
    "        match = re.search(r'solution/[^/]+/(\\w+)', arn)\n",
    "        name = match.group(1)\n",
    "        if name == f\"{resource_name}-solution-version\":\n",
    "            sv_arn = arn\n",
    "            print(f\"Solution Arn: {sv_arn}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0422501-54a8-4577-b29e-777ad33d99a6",
   "metadata": {},
   "source": [
    "#### Wait for creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa90cd9-5d8b-4209-9396-12ec701ccc8d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "max_time = time.time() + 3*60*60 # 3 hours\n",
    "while time.time() < max_time:\n",
    "    describe_sv_response = personalize.describe_solution_version(\n",
    "        solutionVersionArn = sv_arn\n",
    "    )\n",
    "    status = describe_sv_response[\"solutionVersion\"][\"status\"] \n",
    "    print(\"{} : {}\".format(sv_arn, status))\n",
    "    if status == \"ACTIVE\" or status == \"CREATE FAILED\":\n",
    "        break\n",
    "    time.sleep(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c425605-7a36-4676-95d1-f48f55cd7d3f",
   "metadata": {},
   "source": [
    "### Create Campaign\n",
    "\n",
    "Once a solution version is created, it is possible to get recommendations from them, and to get a feel for their overall behavior.\n",
    "\n",
    "For real-time recommendations, after you prepare and import data and creating a solution, you are ready to deploy your solution version to generate recommendations. You deploy a solution version by creating an Amazon Personalize campaign. If you are getting batch recommendations, you don't need to create a campaign. For more information see [Getting batch recommendations and user segments](https://docs.aws.amazon.com/personalize/latest/dg/recommendations-batch.html).\n",
    "\n",
    "A campaign is a hosted solution version; an endpoint which you can query for recommendations. Pricing is set by estimating throughput capacity (requests from users for personalization per second). When deploying a campaign, you set a minimum throughput per second (TPS) value. This service, like many within AWS, will automatically scale based on demand, but if latency is critical, you may want to provision ahead for larger demand. For this POC and demo, all minimum throughput thresholds are set to 1. For more information, see the [pricing page](https://aws.amazon.com/personalize/pricing/).\n",
    "\n",
    "Once we're satisfied with our solution version, we need to create Campaigns for each solution version. When creating a campaign you specify the minimum transactions per second (`minProvisionedTPS`) that you expect to make against the service for this campaign. Personalize will automatically scale the inference endpoint up and down for the campaign to match demand but will never scale below `minProvisionedTPS`.\n",
    "\n",
    "Let's create a campaigns for our solution versions set at `minProvisionedTPS` of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "472eb209-3252-4549-a182-eb1abca3bf23",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    create_campaign_response = personalize.create_campaign(\n",
    "        name = resource_name,\n",
    "        solutionVersionArn = sv_arn)\n",
    "    campaign_arn = create_campaign_response[\"campaignArn\"]\n",
    "    print('campaign arn:', campaign_arn)\n",
    "\n",
    "except personalize.exceptions.ResourceAlreadyExistsException:\n",
    "    print('You already created this Campaign.')\n",
    "    campaigns = personalize.list_campaigns(\n",
    "            solutionArn=solution_arn,\n",
    "            maxResults=100\n",
    "        )\n",
    "    for campaign in campaigns['campaigns']:\n",
    "        if campaign['name'] == resource_name:\n",
    "            campaign_arn = campaign['campaignArn']\n",
    "            print(f\"Campaign Arn: {campaign_arn}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9cb7f4f-5eaa-4be3-a7f2-0c675217bd98",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Wait for creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36972d66-7cac-41e4-bbf5-a2ea4588a5f7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "max_time = time.time() + 3*60*60 # 3 hours\n",
    "while time.time() < max_time:\n",
    "    describe_campaign_response = personalize.describe_campaign(\n",
    "        campaignArn = campaign_arn\n",
    "    )\n",
    "    status = describe_campaign_response[\"campaign\"][\"status\"] \n",
    "    print(\"{} : {}\".format(campaign_arn, status))\n",
    "    if status == \"ACTIVE\" or status == \"CREATE FAILED\":\n",
    "        break\n",
    "    time.sleep(30)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b693b0a2-78dd-4d62-b2c5-d8659b8f03b3",
   "metadata": {},
   "source": [
    "## Storing useful variables \n",
    "Before exiting this notebook, run the following cells to save the version ARNs for use in the next notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8153afc7-1c85-4692-999e-a7dd02127caa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%store campaign_arn\n",
    "%store role_arn\n",
    "%store interactions_filename\n",
    "%store items_filename\n",
    "%store users_filename\n",
    "%store root_dir\n",
    "%store dataset_group_arn\n",
    "%store bucket_name\n",
    "%store region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5c2c5c-6347-4307-8001-1c787f2781df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
